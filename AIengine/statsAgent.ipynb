{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stored 43 player documents in MongoDB.\n",
      "### Relevant Context for jadeja ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo.collection import Collection\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Ensure the Google API key is set\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "mongodb_uri = os.getenv(\"MONGO_URI\")  # Add this to your environment variables\n",
    "\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"Google API key not found in environment variables.\")\n",
    "if not mongodb_uri:\n",
    "    raise ValueError(\"MongoDB URI not found in environment variables.\")\n",
    "\n",
    "def get_mongodb_collection() -> Collection:\n",
    "    \"\"\"Get MongoDB collection instance.\"\"\"\n",
    "    client = MongoClient(mongodb_uri)\n",
    "    db = client['cricket']\n",
    "    return db['players_summaries']\n",
    "\n",
    "def load_player_summaries(file_path: str) -> Dict:\n",
    "    \"\"\"Load player summaries from JSON file.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_player_documents(player_summaries: Dict) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create player documents with embeddings for MongoDB.\n",
    "    \n",
    "    Args:\n",
    "    - player_summaries (dict): Player summaries data.\n",
    "    \n",
    "    Returns:\n",
    "    - list: List of documents with embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    documents = []\n",
    "    \n",
    "    for player_id, player_data in player_summaries.items():\n",
    "        player_name = player_data.get(\"name\", \"Unknown\")\n",
    "        summary = player_data.get(\"summary\", \"No summary available.\")\n",
    "        \n",
    "        # Create the text that will be embedded\n",
    "        text_to_embed = f\"Player ID: {player_id}\\nName: {player_name}\\n\\n{summary}\"\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding_vector = embeddings.embed_query(text_to_embed)\n",
    "        \n",
    "        # Create document\n",
    "        document = {\n",
    "            \"player_id\": player_id,\n",
    "            \"name\": player_name,\n",
    "            \"full_text\": text_to_embed,\n",
    "            \"embedding\": embedding_vector\n",
    "        }\n",
    "        documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def store_player_documents(documents: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Store player documents in MongoDB.\n",
    "    \n",
    "    Args:\n",
    "    - documents (list): List of player documents with embeddings.\n",
    "    \"\"\"\n",
    "    collection = get_mongodb_collection()\n",
    "    \n",
    "    # Drop existing documents and insert new ones\n",
    "    collection.drop()\n",
    "    collection.insert_many(documents)\n",
    "    print(f\"Successfully stored {len(documents)} player documents in MongoDB.\")\n",
    "\n",
    "def search_similar_players(query: str, limit: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for similar players using vector similarity.\n",
    "    \n",
    "    Args:\n",
    "    - query (str): Search query\n",
    "    - limit (int): Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "    - list: List of similar player documents\n",
    "    \"\"\"\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    \n",
    "    collection = get_mongodb_collection()\n",
    "    \n",
    "    # Corrected Vector Search Pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",   # Ensure this matches your MongoDB index name\n",
    "                \"limit\": limit,\n",
    "                \"numCandidates\": 10,\n",
    "                \"path\": \"embedding\",       # This should match the field where embeddings are stored\n",
    "                \"queryVector\": query_embedding\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"player_id\": 1,\n",
    "                \"name\": 1,\n",
    "                \"full_text\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}  # Fix metadata field name\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Update the get_player_context function to use MongoDB\n",
    "def get_player_context(player_query: str, top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get player context using vector similarity search.\n",
    "    \n",
    "    Args:\n",
    "    - player_query (str): Query string\n",
    "    - top_k (int): Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "    - list: List of relevant documents\n",
    "    \"\"\"\n",
    "    results = search_similar_players(player_query, top_k)\n",
    "    \n",
    "    # Convert to format compatible with existing code\n",
    "    from langchain_core.documents import Document\n",
    "    documents = []\n",
    "    for result in results:\n",
    "        doc = Document(\n",
    "            page_content=result['full_text'],\n",
    "            metadata={\"player_id\": result['player_id'], \"name\": result['name']}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load player summaries\n",
    "    file_path = \"player_summaries.json\"\n",
    "    player_summaries = load_player_summaries(file_path)\n",
    "    \n",
    "    # Create and store player documents with embeddings\n",
    "    player_documents = create_player_documents(player_summaries)\n",
    "    store_player_documents(player_documents)\n",
    "    \n",
    "    # Query example\n",
    "    player_query = \"jadeja\"\n",
    "    relevant_contexts = get_player_context(player_query)\n",
    "    \n",
    "    # Print relevant context\n",
    "    print(f\"### Relevant Context for {player_query} ###\\n\")\n",
    "    for doc in relevant_contexts:\n",
    "        print(doc.page_content)\n",
    "        print(\"\\nMetadata:\", doc.metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_player_context() got multiple values for argument 'top_k'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 112\u001b[0m\n\u001b[0;32m    107\u001b[0m questions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    108\u001b[0m     \n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhich bowler has good stats against gaikwad in opponent team?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    110\u001b[0m ]\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[1;32m--> 112\u001b[0m     \u001b[43mprocess_user_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaiss_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Example question\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# user_question = \"Which players performed well in the last five matches?\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# process_user_query(user_question, faiss_index)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 89\u001b[0m, in \u001b[0;36mprocess_user_query\u001b[1;34m(user_question, faiss_index, top_k)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mProcess the user question by retrieving relevant context and querying the AI model.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m- None\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Retrieve relevant context\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[43mget_player_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaiss_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m relevant_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m relevant_docs])\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relevant_context:\n",
      "\u001b[1;31mTypeError\u001b[0m: get_player_context() got multiple values for argument 'top_k'"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "# Initialize the Groq client\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "def query_ai_model(question, context):\n",
    "    \"\"\"\n",
    "    Query the AI model with a user question and relevant context.\n",
    "    \n",
    "    Args:\n",
    "    - question (str): User's question.\n",
    "    - context (str): Relevant context retrieved from FAISS index.\n",
    "    \n",
    "    Returns:\n",
    "    - str: AI-generated response.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert cricket analyst with deep knowledge of the game. Your role is to analyze match data and provide insightful responses to questions.\n",
    "### Guidelines:\n",
    "1. Response Format:\n",
    "   - Begin with a direct answer to the question\n",
    "   - Support your analysis with relevant statistics from the provided match data\n",
    "   - Use cricket terminology appropriately\n",
    "   - Keep responses concise but informative\n",
    "2. Analysis Guidelines:\n",
    "   - When analyzing batting:\n",
    "     * Consider phase-wise performance (powerplay, middle overs, death overs)\n",
    "     * Look at strike rates and boundary percentages\n",
    "     * Analyze dismissal patterns and batting control\n",
    "   - When analyzing bowling:\n",
    "     * Focus on economy rates and wicket-taking ability in different phases\n",
    "     * Consider boundary percentage conceded\n",
    "     * Analyze bowling variations and their effectiveness\n",
    "3. Special Instructions:\n",
    "   - Always reference specific data points to support your analysis\n",
    "   - If asked about comparisons, use actual numbers from the match data\n",
    "   - If data is not available for a specific query, clearly state that\n",
    "   - For trend analysis, consider both basic and detailed statistics\n",
    "   - When discussing match results, reference both venue and head-to-head analysis\n",
    "4. Response Style:\n",
    "   - Be professional but engaging\n",
    "   - Use clear, concise language\n",
    "   - Present complex statistics in an easy-to-understand manner\n",
    "   - Highlight key insights and patterns\n",
    "Remember to maintain objectivity and base all analysis on the provided match data.\n",
    "    ### Context:\n",
    "    {context}\n",
    "    \n",
    "    ### Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            # model=\"mixtral-8x7b-32768\",\n",
    "            model=\"llama3-70b-8192\",\n",
    "            # model=\"llama-3.1-8b-instant\",  # Replace with your model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \n",
    "                 \"content\": '''\n",
    "                    You are an expert cricket analyst. Analyze match data and provide insightful responses.\n",
    "                    Remember to maintain objectivity and base all analysis on the provided match data.'''\n",
    "                 },\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=512\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error querying AI model: {str(e)}\"\n",
    "\n",
    "# Example usage\n",
    "def process_user_query(user_question, faiss_index, top_k=5):\n",
    "    \"\"\"\n",
    "    Process the user question by retrieving relevant context and querying the AI model.\n",
    "    \n",
    "    Args:\n",
    "    - user_question (str): User's question.\n",
    "    - faiss_index: Loaded FAISS index object.\n",
    "    - top_k (int): Number of top relevant chunks to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Retrieve relevant context\n",
    "    relevant_docs = get_player_context(user_question, faiss_index, top_k=top_k)\n",
    "    relevant_context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    if not relevant_context:\n",
    "        print(\"No relevant context found for the question.\")\n",
    "        return\n",
    "\n",
    "    # Query the AI model\n",
    "    ai_response = query_ai_model(user_question, relevant_context)\n",
    "    \n",
    "    # Print the response\n",
    "    print(f\"### User Question:\\n{user_question}\\n\")\n",
    "    print(f\"### AI Response:\\n{ai_response}\\n\")\n",
    "\n",
    "# Load FAISS index (assuming it's already created and saved locally)\n",
    "faiss_index = load_faiss_index()\n",
    "\n",
    "\n",
    "questions = [\n",
    "    \n",
    "    'which bowler has good stats against gaikwad in opponent team?'\n",
    "]\n",
    "for question in questions:\n",
    "    process_user_query(question, faiss_index)\n",
    "\n",
    "# Example question\n",
    "# user_question = \"Which players performed well in the last five matches?\"\n",
    "# process_user_query(user_question, faiss_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cricket",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
