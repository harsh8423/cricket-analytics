{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.0-py3-none-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (2.5.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.1 MB 1.5 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.8/10.1 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/10.1 MB 1.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/10.1 MB 1.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.8/10.1 MB 1.7 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.1/10.1 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.6/10.1 MB 1.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.9/10.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.9/10.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.4/10.1 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 3.7/10.1 MB 1.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 3.9/10.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.7/10.1 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.2/10.1 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.8/10.1 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.3/10.1 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 6.6/10.1 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.1/10.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.6/10.1 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.1/10.1 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.4/10.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.1 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 2.0 MB/s eta 0:00:00\n",
      "Using cached peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Using cached bitsandbytes-0.45.0-py3-none-win_amd64.whl (68.5 MB)\n",
      "Using cached accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.5 MB 3.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.0/11.5 MB 3.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 2.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.4/11.5 MB 2.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.6/11.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.9/11.5 MB 2.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.5/11.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.5/11.5 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.8/11.5 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.8/11.5 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.3/11.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.1/11.5 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.6/11.5 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.6/11.5 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.6/11.5 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.9/11.5 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.9/11.5 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.2/11.5 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.4/11.5 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 2.0 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading scikit_learn-1.6.0-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.1 MB 2.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.8/11.1 MB 2.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.8/11.1 MB 2.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.8/11.1 MB 2.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.8/11.1 MB 2.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.1/11.1 MB 2.0 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.6/11.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.4/11.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.7/11.1 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.2/11.1 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.7/11.1 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.2/11.1 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.8/11.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.3/11.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.6/11.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.1/11.1 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.6/11.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.1/11.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.4/11.1 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.9/11.1 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.2/11.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.4/11.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.1 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.7/11.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 2.0 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/44.5 MB 2.4 MB/s eta 0:00:19\n",
      "    --------------------------------------- 1.0/44.5 MB 2.6 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 1.3/44.5 MB 2.7 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 1.8/44.5 MB 2.6 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 1.8/44.5 MB 2.6 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 2.9/44.5 MB 2.4 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 3.4/44.5 MB 2.4 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 3.9/44.5 MB 2.4 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 4.5/44.5 MB 2.5 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 5.0/44.5 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 5.2/44.5 MB 2.4 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 5.8/44.5 MB 2.4 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 6.3/44.5 MB 2.4 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 6.8/44.5 MB 2.4 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 7.3/44.5 MB 2.4 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 7.9/44.5 MB 2.4 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 8.1/44.5 MB 2.4 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 8.1/44.5 MB 2.4 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 8.9/44.5 MB 2.4 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 9.2/44.5 MB 2.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 9.7/44.5 MB 2.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 10.0/44.5 MB 2.3 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 10.5/44.5 MB 2.2 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 11.0/44.5 MB 2.2 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 11.5/44.5 MB 2.3 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 2.3 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 12.6/44.5 MB 2.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 12.8/44.5 MB 2.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 13.1/44.5 MB 2.3 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 13.6/44.5 MB 2.2 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 13.9/44.5 MB 2.2 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 14.4/44.5 MB 2.2 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 14.7/44.5 MB 2.2 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 15.2/44.5 MB 2.2 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 15.7/44.5 MB 2.2 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 16.3/44.5 MB 2.2 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 16.8/44.5 MB 2.2 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 17.0/44.5 MB 2.2 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 17.6/44.5 MB 2.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 18.1/44.5 MB 2.2 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 18.4/44.5 MB 2.2 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 18.9/44.5 MB 2.2 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 19.1/44.5 MB 2.2 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 19.7/44.5 MB 2.2 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 19.9/44.5 MB 2.2 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 20.2/44.5 MB 2.2 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 20.7/44.5 MB 2.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 21.2/44.5 MB 2.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 21.8/44.5 MB 2.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 22.0/44.5 MB 2.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 22.5/44.5 MB 2.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 23.1/44.5 MB 2.2 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 23.3/44.5 MB 2.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 23.9/44.5 MB 2.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 24.4/44.5 MB 2.2 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 24.9/44.5 MB 2.2 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 25.2/44.5 MB 2.2 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 25.7/44.5 MB 2.2 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 26.2/44.5 MB 2.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 26.7/44.5 MB 2.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 27.3/44.5 MB 2.2 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 27.8/44.5 MB 2.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 28.3/44.5 MB 2.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 28.6/44.5 MB 2.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 28.8/44.5 MB 2.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 29.4/44.5 MB 2.2 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 29.9/44.5 MB 2.2 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 30.1/44.5 MB 2.2 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 30.7/44.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 31.2/44.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 31.5/44.5 MB 2.2 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 32.0/44.5 MB 2.2 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 32.2/44.5 MB 2.2 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 32.2/44.5 MB 2.2 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 32.8/44.5 MB 2.2 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 33.3/44.5 MB 2.2 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 33.6/44.5 MB 2.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 33.8/44.5 MB 2.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 34.1/44.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 34.3/44.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 34.9/44.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 35.1/44.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 35.4/44.5 MB 2.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 35.9/44.5 MB 2.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 35.9/44.5 MB 2.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 35.9/44.5 MB 2.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 36.2/44.5 MB 2.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 36.4/44.5 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 36.7/44.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 37.0/44.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 37.5/44.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 37.7/44.5 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 38.0/44.5 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 38.3/44.5 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 38.8/44.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 39.3/44.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 39.6/44.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 39.8/44.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 40.1/44.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 40.1/44.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 40.1/44.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 41.4/44.5 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 41.7/44.5 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 42.2/44.5 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 42.5/44.5 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 43.0/44.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/44.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.8/44.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 2.0 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, tqdm, threadpoolctl, sympy, scipy, safetensors, regex, joblib, fsspec, scikit-learn, pandas, huggingface-hub, tokenizers, bitsandbytes, accelerate, transformers, peft\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed accelerate-1.2.1 bitsandbytes-0.45.0 fsspec-2024.12.0 huggingface-hub-0.27.0 joblib-1.4.2 pandas-2.2.3 peft-0.14.0 regex-2024.11.6 safetensors-0.4.5 scikit-learn-1.6.0 scipy-1.14.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.47.1 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers peft bitsandbytes accelerate pandas numpy torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commText</th>\n",
       "      <th>overNumber</th>\n",
       "      <th>inning</th>\n",
       "      <th>runs</th>\n",
       "      <th>batsman_name</th>\n",
       "      <th>bowler_name</th>\n",
       "      <th>is_wicket</th>\n",
       "      <th>wicket_type</th>\n",
       "      <th>shot_played</th>\n",
       "      <th>ball_length</th>\n",
       "      <th>ball_line</th>\n",
       "      <th>batting_control</th>\n",
       "      <th>bowling_variation</th>\n",
       "      <th>shot_direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joshua Little to Dhoni, 1 run, CSK finish wit...</td>\n",
       "      <td>19.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MS Dhoni</td>\n",
       "      <td>Joshua Little</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>pull</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>in-control</td>\n",
       "      <td>slower</td>\n",
       "      <td>square</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            commText  overNumber  inning  \\\n",
       "0   Joshua Little to Dhoni, 1 run, CSK finish wit...        19.6       1   \n",
       "\n",
       "   runs batsman_name    bowler_name  is_wicket wicket_type shot_played  \\\n",
       "0     1     MS Dhoni  Joshua Little      False         N/A        pull   \n",
       "\n",
       "  ball_length ball_line batting_control bowling_variation shot_direction  \n",
       "0        None      None      in-control            slower         square  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load JSON file\n",
    "file_path = \"commentarydata.json\"  # Replace with your actual file path\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Normalize both innings\n",
    "inning1 = pd.json_normalize(data, record_path=['commentary_innings1'])\n",
    "inning2 = pd.json_normalize(data, record_path=['commentary_innings2'])\n",
    "\n",
    "# Add a column to distinguish between innings\n",
    "inning1['inning'] = 1\n",
    "inning2['inning'] = 2\n",
    "\n",
    "# Combine both innings into a single DataFrame\n",
    "commentary = pd.concat([inning1, inning2], ignore_index=True)\n",
    "\n",
    "\n",
    "# Function to extract runs scored\n",
    "def extract_runs(comm_text):\n",
    "    if \"SIX\" in comm_text or \"six\" in comm_text:\n",
    "        return 6\n",
    "    elif \"FOUR\" in comm_text or \"four\" in comm_text:\n",
    "        return 4\n",
    "    elif \"1 run\" in comm_text:\n",
    "        return 1\n",
    "    elif \"2 runs\" in comm_text:\n",
    "        return 2\n",
    "    elif \"3 runs\" in comm_text:\n",
    "        return 3\n",
    "    elif \"no run\" in comm_text:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0\n",
    "    return None\n",
    "\n",
    "def wicket_type(comm_text):\n",
    "    # Array of wicket type keywords for easy future additions\n",
    "    wicket_keywords = ['runout', 'stumped', 'lbw', 'hit wicket']\n",
    "    \n",
    "    # Check for special combinations first\n",
    "    text_lower = comm_text.lower()\n",
    "    if 'caught' in text_lower and 'edge' in text_lower:\n",
    "        return 'edge&caught'\n",
    "    elif 'played on' in text_lower and 'bowled' in text_lower:\n",
    "        return 'played_on'\n",
    "    \n",
    "    # Check for individual keywords\n",
    "    if 'caught' in text_lower:\n",
    "        return 'caught'\n",
    "    if 'bowled' in text_lower:\n",
    "        return 'bowled'\n",
    "    \n",
    "    # Check other wicket types\n",
    "    for wicket in wicket_keywords:\n",
    "        if wicket in text_lower:\n",
    "            return wicket\n",
    "    \n",
    "    return 'wicket'\n",
    "\n",
    "def extract_shot(comm_text):\n",
    "    shot_keywords = ['pull', 'cut', 'drive', 'sweep', 'flick', 'hook', 'loft','slog', 'punch','down the ground','across the line','scoop']\n",
    "    if 'goes down' in comm_text.lower() or 'whip' in comm_text.lower():\n",
    "        return 'loft'\n",
    "    if 'nudge' in comm_text.lower() or 'tap' in comm_text.lower() or 'darted' in comm_text.lower() or 'paddle' in comm_text.lower() or 'tuck' in comm_text.lower() or 'pushed' in comm_text.lower() or 'knock' in comm_text.lower() or 'clipped' in comm_text.lower() or 'steer' in comm_text.lower() or 'dab' in comm_text.lower():\n",
    "        return 'pushed slightly'\n",
    "    if 'defend' in comm_text.lower() or 'defence' in comm_text.lower() or 'blocked' in comm_text.lower():\n",
    "        return 'defend'\n",
    "    for shot in shot_keywords:\n",
    "        if shot in comm_text.lower():\n",
    "            return shot\n",
    "    return None\n",
    "\n",
    "def extract_direction(comm_text):\n",
    "    direction_keywords = ['cover', 'square', 'point']\n",
    "    \n",
    "    for direction in direction_keywords:\n",
    "        if direction in comm_text.lower():\n",
    "            return direction\n",
    "    if 'fine leg' in comm_text.lower() or 'fine-leg' in comm_text.lower() or 'fine' in comm_text.lower():\n",
    "        return 'fine-leg'\n",
    "    if 'third-man' in comm_text.lower() or 'third man' in comm_text.lower():\n",
    "        return 'third-man'\n",
    "    if 'mid-wicket' in comm_text.lower() or 'midwicket' in comm_text.lower() or 'mid wicket' in comm_text.lower():\n",
    "        return 'mid-wicket'\n",
    "    if 'mid-on' in comm_text.lower() or 'mid on' in comm_text.lower():\n",
    "        return 'mid-on'\n",
    "    if 'mid-off' in comm_text.lower() or 'mid off' in comm_text.lower():\n",
    "        return 'mid-off'\n",
    "    if 'long-on' in comm_text.lower() or 'long on' in comm_text.lower():\n",
    "        return 'long-on'\n",
    "    if 'long-off' in comm_text.lower() or 'long off' in comm_text.lower():\n",
    "        return 'long-off'\n",
    "    if 'defend' in comm_text.lower() or 'defence' in comm_text.lower() or 'blocked' in comm_text.lower() or 'wide' in comm_text.lower():\n",
    "        return 'blocked'\n",
    "    if  'keeper' in comm_text.lower():\n",
    "        return 'keeper'\n",
    "    return None\n",
    "\n",
    "def extract_length(comm_text):\n",
    "    length_keywords = ['short', 'full', 'yorker', 'bouncer','half-volley','good length','tossed up']\n",
    "    \n",
    "    if 'full toss' in comm_text.lower() or 'full-toss' in comm_text.lower():\n",
    "        return 'full-toss'\n",
    "    for length in length_keywords:\n",
    "        if length in comm_text.lower():\n",
    "            return length\n",
    "    if 'back of a length' in comm_text.lower() or 'short of a length' in comm_text.lower():\n",
    "        return 'back of a length'\n",
    "    elif 'length delivery' in comm_text.lower() or 'length ball' in comm_text.lower():\n",
    "        return 'length ball'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_ball_line(comm_text):\n",
    "    text_lower = comm_text.lower()\n",
    "    line_keywords = {\n",
    "        'outside-off': ['outside off', 'outside-off'],\n",
    "        'off-stump': ['off stump line', 'off-stump', 'off stump', 'around off','middle and off'],\n",
    "        'middle-stump': ['middle stump', 'middle-stump','on middle', 'on stump', 'around middle','middle and leg', 'body'],\n",
    "        'leg-stump': ['leg stump', 'leg-stump', 'outside leg','on leg','leg bye', 'around leg'],\n",
    "        'down-leg': ['down leg', 'down-leg'],\n",
    "        'wide': ['wide']\n",
    "    }\n",
    "    variations = ['seam', 'swing', 'seam-away', 'seam-in', 'seam-up',]\n",
    "    for line, variations in line_keywords.items():\n",
    "        for variation in variations:\n",
    "            if variation in text_lower:\n",
    "                return line\n",
    "        if ('swing' not in text_lower or 'seam' not in text_lower) and 'pads' in text_lower:\n",
    "            return 'leg-stump'\n",
    "    return None\n",
    "\n",
    "def extract_batting_control(row):\n",
    "    text_lower = row['commText'].lower()\n",
    "    control_keywords = {\n",
    "        'no-control': ['missed', 'misses', 'edge', 'edges'],\n",
    "        'beaten': ['beaten', 'beats']\n",
    "    }\n",
    "\n",
    "    if row['event'] == 'WICKET':\n",
    "        return 'no-control'\n",
    "    \n",
    "    for control, keywords in control_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in text_lower:\n",
    "                return control\n",
    "    return 'in-control'\n",
    "\n",
    "\n",
    "\n",
    "def extract_bowling_variation(comm_text):\n",
    "    text_lower = comm_text.lower()\n",
    "    \n",
    "    # Check for combined variations first\n",
    "    if 'seam' in text_lower and 'away' in text_lower:\n",
    "        return 'seam-away'\n",
    "    if 'seaming' in text_lower and 'in' in text_lower:\n",
    "        return 'seam-in'\n",
    "    \n",
    "    variation_keywords = {\n",
    "        'seam-up': ['seam up', 'seam-up'],\n",
    "        'out-swing': ['swinging away','outswinger'],\n",
    "        'in-swing': ['swinging in','inswinger'],\n",
    "        'swing': ['swinging'],\n",
    "        'googly': ['googly'],\n",
    "        'slower': ['slower', 'lack of pace'],\n",
    "        'cutter': ['cutter'],\n",
    "        'yorker': ['yorker'],\n",
    "        'bouncer': ['bouncer'],\n",
    "        'off-break': ['off-break'],\n",
    "        'carrom ball': ['carrom'],\n",
    "        'flipper': ['flipper'],\n",
    "        'leg-break': ['leg-break'],\n",
    "        'off-spin': ['off-spin'],\n",
    "        'leg-spin': ['leg spin'],\n",
    "        'knuckle ball': ['knuckle ball'],\n",
    "        'quick': ['quick'],\n",
    "        'arm ball': ['arm ball'],\n",
    "        'slider':['slider'],\n",
    "        'skidding':['skidding', 'skidded']\n",
    "\n",
    "    }\n",
    "    \n",
    "    for variation, keywords in variation_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in text_lower:\n",
    "                return variation\n",
    "    return 'normal'\n",
    "\n",
    "# Apply all transformations\n",
    "def process_commentary(commentary):\n",
    "    # Extract all required information\n",
    "    commentary['runs'] = commentary['commText'].apply(extract_runs)\n",
    "    \n",
    "    # Extract player details before removing the original columns\n",
    "    commentary['batsman_name'] = commentary['batsman.batName']\n",
    "    commentary['bowler_name'] = commentary['bowler.bowlName']\n",
    "    commentary['batsman_id'] = commentary['batsman.batId']\n",
    "    commentary['bowler_id'] = commentary['bowler.bowlId']\n",
    "    \n",
    "    # Identify if it's a wicket ball\n",
    "    commentary['is_wicket'] = commentary['event'] == 'WICKET'\n",
    "    \n",
    "    # Apply wicket type only if it's a wicket ball\n",
    "    commentary['wicket_type'] = commentary.apply(\n",
    "        lambda row: wicket_type(row['commText']) if row['event'] == 'WICKET' else 'N/A',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extract shot, length, line, control and variation information\n",
    "    commentary['shot_played'] = commentary['commText'].apply(extract_shot)\n",
    "    commentary['ball_length'] = commentary['commText'].apply(extract_length)\n",
    "    commentary['ball_line'] = commentary['commText'].apply(extract_ball_line)\n",
    "    # Apply this updated function\n",
    "    commentary['batting_control'] = commentary.apply(extract_batting_control, axis=1)\n",
    "    commentary['bowling_variation'] = commentary['commText'].apply(extract_bowling_variation)\n",
    "    commentary['shot_direction'] = commentary['commText'].apply(extract_direction)\n",
    "    \n",
    "    # Remove unwanted columns\n",
    "    columns_to_remove = [\n",
    "        'batsman.batId',\n",
    "        'batsman.batName',\n",
    "        'bowler.bowlId',\n",
    "        'bowler.bowlName',\n",
    "        'event',\n",
    "        # 'batsman_name',\n",
    "        # 'bowler_name',\n",
    "        'batsman_id',\n",
    "        'bowler_id',\n",
    "        'batTeamName'\n",
    "    ]\n",
    "    commentary.drop(columns=columns_to_remove, inplace=True)\n",
    "    \n",
    "    return commentary\n",
    "\n",
    "# Process the commentary DataFrame\n",
    "commentary = process_commentary(commentary)\n",
    "commentary.head(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in commentary_before: 9083\n",
      "Number of rows in refined_commentary: 21012\n"
     ]
    }
   ],
   "source": [
    "def refine_commentary_fields(commentary):\n",
    "    \"\"\"\n",
    "    Refine values in the filtered commentary based on specific cricket-related conditions\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    refined_df = commentary.copy()\n",
    "    \n",
    "    # Apply condition 1: Update shot_direction based on batting control and wicket type\n",
    "    mask_control = (refined_df['batting_control'] != 'in-control') & (refined_df['shot_direction'].isnull())\n",
    "    mask_stumped = refined_df['wicket_type'] == 'stumped'\n",
    "    refined_df.loc[mask_control | mask_stumped, 'shot_direction'] = 'keeper'\n",
    "    \n",
    "    # Apply condition 2: Update shot_played based on direction and runs\n",
    "    mask_shot_null = refined_df['shot_played'].isnull() & refined_df['shot_direction'].notnull()\n",
    "    mask_low_runs = refined_df['runs'].isin([0, 1, 2])\n",
    "    refined_df.loc[mask_shot_null & mask_low_runs, 'shot_played'] = 'pushed slightly'\n",
    "    refined_df.loc[mask_shot_null & ~mask_low_runs, 'shot_played'] = 'normal'\n",
    "    \n",
    "    # Apply condition 3: Update ball_line based on shot direction and bowling variation\n",
    "    mask_offside_direction = refined_df['shot_direction'].isin(['cover', 'point', 'third-man'])\n",
    "    mask_not_away = ~refined_df['bowling_variation'].isin(['seam-away', 'out-swing'])\n",
    "    mask_valid_line = refined_df['ball_line'].isnull()\n",
    "    refined_df.loc[mask_valid_line & mask_offside_direction & mask_not_away, 'ball_line'] = 'outside-off'\n",
    "    \n",
    "    # Apply condition 4: Update ball_length based on multiple conditions\n",
    "    # Condition 4.1: outside-off drive\n",
    "    mask_outside_off_drive = (refined_df['ball_line'] == 'outside-off') & (refined_df['shot_played'] == 'drive')\n",
    "    \n",
    "    # Condition 4.2: specific shots\n",
    "    mask_specific_shots = refined_df['shot_played'].isin(['slog', 'scoop', 'sweep'])\n",
    "    \n",
    "    # Condition 4.3: swing variations\n",
    "    mask_swing = refined_df['bowling_variation'].isin(['in-swing', 'out-swing'])\n",
    "    \n",
    "    # Condition 4.4: point/third-man cut\n",
    "    mask_cut_direction = refined_df['shot_direction'].isin(['point', 'third-man'])\n",
    "    mask_cut_shot = refined_df['shot_played'] == 'cut'\n",
    "    \n",
    "    # Apply length updates\n",
    "    mask_valid_length = refined_df['ball_length'].isnull()\n",
    "    refined_df.loc[mask_valid_length & (mask_outside_off_drive | mask_specific_shots | mask_swing), 'ball_length'] = 'full'\n",
    "    refined_df.loc[mask_valid_length & mask_cut_direction & mask_cut_shot, 'ball_length'] = 'back of a length'\n",
    "    \n",
    "    # Apply condition 5: Update ball_line based on additional conditions\n",
    "    # Condition 5.1: off-stump line\n",
    "    mask_off_direction = refined_df['shot_direction'].isin(['mid-off', 'long-off'])\n",
    "    mask_pushed = refined_df['shot_played'] == 'pushed slightly'\n",
    "    mask_not_short = refined_df['ball_length'] != 'short'\n",
    "    mask_valid_line = refined_df['ball_line'].isnull()\n",
    "    refined_df.loc[mask_valid_line & mask_off_direction & mask_pushed & mask_not_short, 'ball_line'] = 'off-stump'\n",
    "    \n",
    "    # Condition 5.2: leg-stump line for fine-leg direction\n",
    "    mask_fine_leg = refined_df['shot_direction'] == 'fine-leg'\n",
    "    mask_leg_shots = refined_df['shot_played'].isin(['pull', 'flick', 'sweep'])\n",
    "    refined_df.loc[mask_valid_line & mask_fine_leg & mask_leg_shots, 'ball_line'] = 'leg-stump'\n",
    "    \n",
    "    # Condition 5.3: leg-stump line for square direction\n",
    "    mask_square = refined_df['shot_direction'] == 'square'\n",
    "    mask_not_short_back = ~refined_df['ball_length'].isin(['short', 'back of a length'])\n",
    "    refined_df.loc[mask_valid_line & mask_square & mask_not_short_back, 'ball_line'] = 'leg-stump'\n",
    "    \n",
    "    return refined_df\n",
    "\n",
    "# Apply the refinements to the filtered commentary\n",
    "refined_commentary = refine_commentary_fields(commentary)\n",
    "\n",
    "\n",
    "# Filter rows where specific columns are not None\n",
    "commentary_before = commentary[\n",
    "    # (commentary['bowler_id']==265)\n",
    "    # (commentary['batsman_id']==1413)\n",
    "    (commentary['shot_played'].notnull())&\n",
    "    (commentary['ball_length'].notnull())&\n",
    "    (commentary['ball_line'].notnull())&\n",
    "    # (commentary['batting_control'] != 'in-control')&\n",
    "    # (commentary['bowling_variation'] !='normal')\n",
    "    (commentary['shot_direction'].notnull())\n",
    "    # ((commentary['runs']==0) | (commentary['runs']==1) | (commentary['runs']==2)) & \n",
    "    # (commentary['is_wicket']!=True)\n",
    "]\n",
    "\n",
    "\n",
    "# Filter rows where specific columns are not None\n",
    "commentary_after = refined_commentary[\n",
    "    # (refined_commentary['bowler_id']==265)\n",
    "    # (refined_commentary['batsman_id']==1413)\n",
    "    (refined_commentary['shot_played'].notnull())&\n",
    "    (refined_commentary['ball_length'].notnull())&\n",
    "    (refined_commentary['ball_line'].notnull())&\n",
    "    # (refined_commentary['batting_control'].notnull)\n",
    "#     (refined_commentary['bowling_variation'] !='normal')\n",
    "    (refined_commentary['shot_direction'].notnull())\n",
    "#     ((refined_commentary['runs']==0) | (refined_commentary['runs']==1) | (refined_commentary['runs']==2)) & \n",
    "#     (refined_commentary['is_wicket']!=True)\n",
    "]\n",
    "\n",
    "# Print the filtered rows\n",
    "row_count = len(commentary_before)\n",
    "print(f\"Number of rows in commentary_before: {row_count}\")\n",
    "\n",
    "# Print the filtered rows\n",
    "row_count = len(commentary_after)\n",
    "print(f\"Number of rows in refined_commentary: {row_count}\")\n",
    "# commentary_after.tail(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluateNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from evaluate) (2.0.1)\n",
      "Collecting dill (from evaluate)\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from evaluate) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate)\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill (from evaluate)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets>=2.0.0->evaluate)\n",
      "  Downloading aiohttp-3.11.11-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Downloading aiohttp-3.11.11-cp312-cp312-win_amd64.whl (437 kB)\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/25.1 MB 2.5 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.6/25.1 MB 2.8 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 3.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 3.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 3.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 3.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 3.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 3.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 3.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 3.9 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.4/25.1 MB 805.4 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 3.7/25.1 MB 813.9 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 3.7/25.1 MB 813.9 kB/s eta 0:00:27\n",
      "   ------ --------------------------------- 4.2/25.1 MB 876.9 kB/s eta 0:00:24\n",
      "   --------- ------------------------------ 6.0/25.1 MB 1.2 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 6.0/25.1 MB 1.2 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 6.8/25.1 MB 1.3 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 6.8/25.1 MB 1.3 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 6.8/25.1 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 7.1/25.1 MB 1.2 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 7.3/25.1 MB 1.2 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 7.6/25.1 MB 1.2 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 7.6/25.1 MB 1.2 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 7.9/25.1 MB 1.2 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 8.1/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 8.1/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 8.4/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 8.7/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.7/25.1 MB 1.0 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 9.7/25.1 MB 1.0 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 9.7/25.1 MB 1.0 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 10.0/25.1 MB 974.5 kB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 10.2/25.1 MB 979.4 kB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 10.5/25.1 MB 993.0 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 11.3/25.1 MB 1.0 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 12.3/25.1 MB 1.1 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 13.9/25.1 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.7/25.1 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 14.7/25.1 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 14.7/25.1 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 14.7/25.1 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 14.7/25.1 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 14.7/25.1 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 14.7/25.1 MB 1.3 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.2/25.1 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.5/25.1 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.5/25.1 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.7/25.1 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 16.0/25.1 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 16.3/25.1 MB 1.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.8/25.1 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.8/25.1 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 1.3 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.9/25.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.2/25.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.2/25.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.2/25.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.2/25.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.2/25.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.2/25.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.2/25.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.4/25.1 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 20.4/25.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.7/25.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 21.0/25.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 21.0/25.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 21.2/25.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 21.2/25.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.5/25.1 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 21.8/25.1 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 21.8/25.1 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.5/25.1 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.3/25.1 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.1/25.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-18.1.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import evaluate\n",
    "import gc\n",
    "import os\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "class CricketDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):  # Reduced max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def create_query_answer_pairs(self, row):\n",
    "        \"\"\"Create multiple query-answer pairs from a single commentary row\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        # Template 1: Shot played query\n",
    "        shot_query = f\"what shot did {row['batsman_name']} play?\"\n",
    "        shot_answer = f\"{row['batsman_name']} played a {row['shot_played']} towards {row['shot_direction']}\"\n",
    "        pairs.append((shot_query, shot_answer))\n",
    "        \n",
    "        # Template 2: Ball details query\n",
    "        ball_query = f\"how did {row['bowler_name']} bowl to {row['batsman_name']}?\"\n",
    "        ball_answer = f\"{row['bowler_name']} bowled a {row['ball_length']} length {row['ball_line']} line delivery\"\n",
    "        pairs.append((ball_query, ball_answer))\n",
    "        \n",
    "        # Template 3: Result query\n",
    "        result_query = f\"what was the result when {row['bowler_name']} bowled to {row['batsman_name']}?\"\n",
    "        result = f\"Wicket - {row['wicket_type']}\" if row['is_wicket'] else f\"{row['runs']} runs\"\n",
    "        result_answer = f\"The result was {result} with {row['batting_control']}% control\"\n",
    "        pairs.append((result_query, result_answer))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        pairs = self.create_query_answer_pairs(row)\n",
    "        \n",
    "        query, answer = pairs[np.random.randint(0, len(pairs))]\n",
    "        input_text = f\"Q: {query}\"  # Simplified prefix\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            answer,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "def prepare_cricket_data(commentary_data):\n",
    "    \"\"\"Prepare cricket commentary data for training\"\"\"\n",
    "    commentary_data = commentary_data.dropna()\n",
    "    commentary_data['is_wicket'] = commentary_data['is_wicket'].astype(str)\n",
    "    \n",
    "    train_data, val_data = train_test_split(\n",
    "        commentary_data, \n",
    "        test_size=0.15,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': result['rouge1'],\n",
    "        'rouge2': result['rouge2'],\n",
    "        'rougeL': result['rougeL']\n",
    "    }\n",
    "\n",
    "def create_peft_config():\n",
    "    \"\"\"Create LoRA configuration for PEFT\"\"\"\n",
    "    return LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=4,  # Reduced rank\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q\", \"v\"]\n",
    "    )\n",
    "\n",
    "def train_cricket_model(commentary_data, output_dir=\"./cricket_model\"):\n",
    "    \"\"\"Main function to train the model\"\"\"\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Initialize tokenizer and model using a smaller model\n",
    "    model_name = \"google/flan-t5-small\"  # Much smaller than T5-base\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,  # Use 8-bit quantization instead of 4-bit\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Prepare model for training\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "    \n",
    "    # Add LoRA adapters\n",
    "    peft_config = create_peft_config()\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_data, val_data = prepare_cricket_data(commentary_data)\n",
    "    train_dataset = CricketDataset(train_data, tokenizer)\n",
    "    val_dataset = CricketDataset(val_data, tokenizer)\n",
    "    \n",
    "    # Define training arguments with reduced batch sizes and gradient accumulation\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=2,  # Reduced epochs\n",
    "        per_device_train_batch_size=1,  # Reduced batch size\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,  # Increased gradient accumulation\n",
    "        warmup_steps=50,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=1,  # Keep only the best model\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer)\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "class CricketQASystem:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Initialize QA system with PEFT model\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load the base model first\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            \"google/flan-t5-small\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_8bit=True\n",
    "        )\n",
    "        \n",
    "        # Load the PEFT model\n",
    "        self.model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        self.model.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def answer_question(self, question, max_length=50):  # Reduced max_length\n",
    "        \"\"\"Generate answer for a given question\"\"\"\n",
    "        input_text = f\"Q: {question}\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=128,  # Reduced max_length\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=2,  # Reduced beam size\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate usage\"\"\"\n",
    "    try:\n",
    "        # Train the model\n",
    "        model, tokenizer = train_cricket_model(refined_commentary)\n",
    "        \n",
    "        # Create QA system\n",
    "        qa_system = CricketQASystem(\"./cricket_model\")\n",
    "        \n",
    "        # Test questions\n",
    "        test_questions = [\n",
    "            \"which shot does Virat Kohli play most?\",\n",
    "            \"how does Virat Kohli play against short balls?\",\n",
    "            \"what is the average control percentage for Rohit Sharma?\",\n",
    "            \"describe the bowling style of Jasprit Bumrah\"\n",
    "        ]\n",
    "        \n",
    "        # Test the system\n",
    "        for question in test_questions:\n",
    "            try:\n",
    "                answer = qa_system.answer_question(question)\n",
    "                print(f\"\\nQ: {question}\")\n",
    "                print(f\"A: {answer}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing question: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import evaluate\n",
    "# from datasets import load_metric\n",
    "import gc\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "class CricketDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def create_query_answer_pairs(self, row):\n",
    "        \"\"\"Create multiple query-answer pairs from a single commentary row\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        # Template 1: Shot played query\n",
    "        shot_query = f\"what shot did {row['batsman_name']} play?\"\n",
    "        shot_answer = f\"{row['batsman_name']} played a {row['shot_played']} towards {row['shot_direction']}\"\n",
    "        pairs.append((shot_query, shot_answer))\n",
    "        \n",
    "        # Template 2: Ball details query\n",
    "        ball_query = f\"how did {row['bowler_name']} bowl to {row['batsman_name']}?\"\n",
    "        ball_answer = f\"{row['bowler_name']} bowled a {row['ball_length']} length {row['ball_line']} line delivery\"\n",
    "        pairs.append((ball_query, ball_answer))\n",
    "        \n",
    "        # Template 3: Result query\n",
    "        result_query = f\"what was the result when {row['bowler_name']} bowled to {row['batsman_name']}?\"\n",
    "        result = f\"Wicket - {row['wicket_type']}\" if row['is_wicket'] else f\"{row['runs']} runs\"\n",
    "        result_answer = f\"The result was {result} with {row['batting_control']}% control\"\n",
    "        pairs.append((result_query, result_answer))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        pairs = self.create_query_answer_pairs(row)\n",
    "        \n",
    "        query, answer = pairs[np.random.randint(0, len(pairs))]\n",
    "        input_text = f\"answer cricket question: {query}\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            answer,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "def prepare_cricket_data(commentary_data):\n",
    "    \"\"\"Prepare cricket commentary data for training\"\"\"\n",
    "    commentary_data = commentary_data.dropna()\n",
    "    commentary_data['is_wicket'] = commentary_data['is_wicket'].astype(str)\n",
    "    \n",
    "    train_data, val_data = train_test_split(\n",
    "        commentary_data, \n",
    "        test_size=0.15,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "def compute_metrics(eval_pred,tokenizer):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    rouge = evaluate.load('rouge')\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': result['rouge1'],\n",
    "        'rouge2': result['rouge2'],\n",
    "        'rougeL': result['rougeL']\n",
    "    }\n",
    "\n",
    "def create_peft_config():\n",
    "    \"\"\"Create LoRA configuration for PEFT\"\"\"\n",
    "    return LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,  # Rank\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q\", \"v\"]  # Target q and v matrices in attention\n",
    "    )\n",
    "\n",
    "def train_cricket_model(commentary_data, output_dir=\"./cricket_T5_model\"):\n",
    "    \"\"\"Main function to train the model using PEFT/LoRA with memory optimizations\"\"\"\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Initialize quantization config with more aggressive memory savings\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True  # Enable double quantization\n",
    "    )\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"T5-small\")\n",
    "    base_model = T5ForConditionalGeneration.from_pretrained(\n",
    "        \"T5-small\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16  # Ensure model is in float16\n",
    "    )\n",
    "    \n",
    "    # Prepare model for k-bit training\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "    \n",
    "    # Create more memory-efficient LoRA config\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=4,  # Reduced rank\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q\", \"v\"]\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    \n",
    "    # Prepare smaller data samples if needed\n",
    "    train_data, val_data = prepare_cricket_data(commentary_data)\n",
    "    # Optionally reduce dataset size for testing\n",
    "    # train_data = train_data.head(1000)  # Uncomment to use smaller dataset\n",
    "    # val_data = val_data.head(100)\n",
    "    \n",
    "    train_dataset = CricketDataset(train_data, tokenizer)\n",
    "    val_dataset = CricketDataset(val_data, tokenizer)\n",
    "    \n",
    "    # Define training arguments with reduced memory footprint\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,  # Reduced batch size\n",
    "        per_device_eval_batch_size=1,   # Reduced batch size\n",
    "        gradient_accumulation_steps=8,   # Increased gradient accumulation\n",
    "        warmup_steps=50,                # Reduced warmup steps\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,                 # Reduced evaluation frequency\n",
    "        save_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=1,             # Keep only one checkpoint\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_32bit\",      # Memory-efficient optimizer\n",
    "        max_grad_norm=0.3,              # Add gradient clipping\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer with memory optimizations\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer)\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "class CricketQASystem:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Initialize QA system with PEFT model\"\"\"\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load the PEFT model\n",
    "        config = PeftModel.from_pretrained(model_path)\n",
    "        base_model = T5ForConditionalGeneration.from_pretrained(\n",
    "            \"T5-small\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True\n",
    "        )\n",
    "        self.model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        self.model.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def answer_question(self, question, max_length=100):\n",
    "        \"\"\"Generate answer for a given question\"\"\"\n",
    "        input_text = f\"answer cricket question: {question}\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=512,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move inputs to the same device as model\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate usage\"\"\"\n",
    "    try:\n",
    "        # Train the model\n",
    "        model, tokenizer = train_cricket_model(refined_commentary)\n",
    "        \n",
    "        # Create QA system\n",
    "        qa_system = CricketQASystem(\"./cricket_T5_model\")\n",
    "        \n",
    "        # Test questions\n",
    "        test_questions = [\n",
    "            \"which shot does Virat Kohli play most?\",\n",
    "            \"how does Virat Kohli play against short balls?\",\n",
    "            \"what is the average control percentage for Rohit Sharma?\",\n",
    "            \"describe the bowling style of Jasprit Bumrah\"\n",
    "        ]\n",
    "        \n",
    "        # Test the system\n",
    "        for question in test_questions:\n",
    "            try:\n",
    "                answer = qa_system.answer_question(question)\n",
    "                print(f\"\\nQ: {question}\")\n",
    "                print(f\"A: {answer}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing question: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training failed due to memory error: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Try reducing max_samples or batch size further\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Set environment variable for memory management\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "class CricketDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=256):  # Reduced max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def create_query_answer_pairs(self, row):\n",
    "        \"\"\"Create multiple query-answer pairs from a single commentary row\"\"\"\n",
    "        pairs = []\n",
    "\n",
    "        # Template 1: Shot played query\n",
    "        shot_query = f\"what shot did {row['batsman_name']} play?\"\n",
    "        shot_answer = f\"{row['batsman_name']} played a {row['shot_played']} towards {row['shot_direction']}\"\n",
    "        pairs.append((shot_query, shot_answer))\n",
    "\n",
    "        # Template 2: Ball details query\n",
    "        ball_query = f\"how did {row['bowler_name']} bowl to {row['batsman_name']}?\"\n",
    "        ball_answer = f\"{row['bowler_name']} bowled a {row['ball_length']} length {row['ball_line']} line delivery\"\n",
    "        pairs.append((ball_query, ball_answer))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        pairs = self.create_query_answer_pairs(row)\n",
    "\n",
    "        query, answer = pairs[np.random.randint(0, len(pairs))]\n",
    "        input_text = f\"answer cricket question: {query}\"\n",
    "\n",
    "        # Tokenize with smaller max_length\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        targets = self.tokenizer(\n",
    "            answer,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "def prepare_cricket_data(commentary_data, max_samples=None):\n",
    "    \"\"\"Prepare cricket commentary data for training with option to limit samples\"\"\"\n",
    "    commentary_data = commentary_data.dropna()\n",
    "    commentary_data['is_wicket'] = commentary_data['is_wicket'].astype(str)\n",
    "\n",
    "    if max_samples and len(commentary_data) > max_samples:\n",
    "        commentary_data = commentary_data.sample(n=max_samples, random_state=42)\n",
    "\n",
    "    train_data, val_data = train_test_split(\n",
    "        commentary_data,\n",
    "        test_size=0.15,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "def compute_metrics(eval_pred,tokenizer):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Simple accuracy metric\n",
    "    return {\n",
    "        'accuracy': sum(p == l for p, l in zip(decoded_preds, decoded_labels)) / len(decoded_preds)\n",
    "    }\n",
    "\n",
    "def train_cricket_model(commentary_data, output_dir=\"./cricket_t5_model\", max_samples=1000):\n",
    "    \"\"\"Memory-optimized training function\"\"\"\n",
    "\n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Initialize smaller model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")  # Using t5-small instead of t5-base\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "    # Prepare limited data\n",
    "    train_data, val_data = prepare_cricket_data(commentary_data, max_samples=max_samples)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CricketDataset(train_data, tokenizer)\n",
    "    val_dataset = CricketDataset(val_data, tokenizer)\n",
    "\n",
    "    # Memory-optimized training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,  # Reduced batch size\n",
    "        per_device_eval_batch_size=2,   # Reduced batch size\n",
    "        gradient_accumulation_steps=4,   # Accumulate gradients\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_steps=200,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,             # Keep only 2 checkpoints\n",
    "        fp16=True,                      # Use mixed precision training\n",
    "        dataloader_num_workers=0,       # Reduce worker processes\n",
    "        gradient_checkpointing=True     # Enable gradient checkpointing\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer)\n",
    ")\n",
    "\n",
    "    # Train with memory optimization\n",
    "    try:\n",
    "        trainer.train()\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed with error: {str(e)}\")\n",
    "        # Try to free memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        raise e\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "class CricketQASystem:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "        # Move model to CPU if GPU is out of memory\n",
    "        if torch.cuda.is_available() and torch.cuda.memory_allocated() > 0.8 * torch.cuda.get_device_properties(0).total_memory:\n",
    "            self.model = self.model.cpu()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()  # Disable gradient computation during inference\n",
    "    def answer_question(self, question, max_length=50):  # Reduced max_length\n",
    "        input_text = f\"answer cricket question: {question}\"\n",
    "\n",
    "        # Clear cache before inference\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=256,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Move to CPU if necessary\n",
    "        if torch.cuda.is_available() and torch.cuda.memory_allocated() > 0.8 * torch.cuda.get_device_properties(0).total_memory:\n",
    "            inputs = {k: v.cpu() for k, v in inputs.items()}\n",
    "            self.model = self.model.cpu()\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=2,  # Reduced beam size\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Usage example with memory optimization\n",
    "def main():\n",
    "    # Set smaller initial sample size\n",
    "    max_samples = 1000 # Adjust based on your GPU memory\n",
    "    \n",
    "\n",
    "    try:\n",
    "        # Train with memory optimization\n",
    "        model, tokenizer = train_cricket_model(\n",
    "            refined_commentary,\n",
    "            max_samples=max_samples\n",
    "        )\n",
    "\n",
    "        # Create QA system\n",
    "        qa_system = CricketQASystem(\"./cricket_t5_model\")\n",
    "\n",
    "        # Test with basic questions\n",
    "        test_questions = [\n",
    "            \"which shot does Virat Kohli play most?\",\n",
    "            \"how does Virat Kohli play against short balls?\"\n",
    "        ]\n",
    "\n",
    "        for question in test_questions:\n",
    "            try:\n",
    "                answer = qa_system.answer_question(question)\n",
    "                print(f\"\\nQ: {question}\")\n",
    "                print(f\"A: {answer}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing question: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Training failed due to memory error: {str(e)}\")\n",
    "        print(\"Try reducing max_samples or batch size further\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\cricket\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
